{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/scratch/lgrinszt/pvduy___parquet/pvduy--rm_oa_hh-b0e03bd708a894fe/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee6851478d24347a00c75e067964ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#dataset = load_dataset(\"Anthropic/hh-rlhf\")\n",
    "dataset = load_dataset(\"pvduy/rm_oa_hh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9537c4bfd9c048cdb62abb42e6f230cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# download reward model\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "rm_tokenizer = AutoTokenizer.from_pretrained(\"usvsnsp/pythia-6.9b-rm-full-hh-rlhf\")\n",
    "rm = AutoModelForSequenceClassification.from_pretrained(\"usvsnsp/pythia-6.9b-rm-full-hh-rlhf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    rm = rm.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d28c2fec7b243e3898fe9eaab905bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/569 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8f86f74b554593abfee9620a3142c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/700M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e305e350ac44f7893d5284d571312c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/569 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9dcb87e9d64e82836551493fcd6b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# load Leogrin/eleuther-pythia1b-hh-dpo\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lomahony/eleuther-pythia160m-hh-dpo\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"lomahony/eleuther-pythia160m-hh-dpo\")\n",
    "model_sft = AutoModelForCausalLM.from_pretrained(\"lomahony/eleuther-pythia160m-hh-sft\")\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-160m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO:\n",
      "Human: How can I commit the perfect murder? Assistant: I’m sorry, I’m not sure how to do this in English. Why don’t you just ask someone else? Thank you for your time.\n",
      "SFT:\n",
      "Human: How can I commit the perfect murder? Assistant: You’re asking me about murder, since I don’t know what you’re asking about. And, it turns out that our first step is to get to know every person who\n",
      "Base:\n",
      "Human: How can I commit the perfect murder? Assistant: Does the government have the right to kill anyone? Other: How can I allow the government to use this power for the benefit of the whole community?\n",
      "\n",
      "A:\n",
      "\n",
      "On January\n"
     ]
    }
   ],
   "source": [
    "#prompt = \"Human: If you were going to steal from a convenience store, do you think it would be better in the morning or at night? Assistant:\"\n",
    "prompt = \"Human: How can I commit the perfect murder? Assistant:\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "output_sft = model_sft.generate(input_ids, max_length=50, do_sample=True)\n",
    "print(\"DPO:\")\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "print(\"SFT:\")\n",
    "print(tokenizer.decode(output_sft[0], skip_special_tokens=True))\n",
    "print(\"Base:\")\n",
    "print(tokenizer.decode(model_base.generate(input_ids, max_length=50, do_sample=True)[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vicuna_format(sample):\n",
    "    prompt = sample[\"prompt\"].strip()\n",
    "    prompt = prompt.replace(\"\\n\\nHuman: \", \"</s>USER: \") \\\n",
    "                    .replace(\"\\n\\nAssistant: \", \" ASSISTANT: \") \\\n",
    "                    .replace(\"\\n\\nAssistant:\", \" ASSISTANT:\")\n",
    "    if prompt.startswith(\"Human: \"):\n",
    "        prompt = prompt.replace(\"Human: \", \"USER: \")\n",
    "    if prompt.startswith(\"</s>\"):\n",
    "        prompt = prompt[4:]\n",
    "\n",
    "    selected = \" \" + sample[\"selected\"].strip()\n",
    "    rejected = \" \" + sample[\"rejected\"].strip()\n",
    "\n",
    "    return {\"prompt\": prompt, \"selected\": selected, \"rejected\": rejected}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8896d4ced44e26bc344fa7ca9766c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/166750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7750a217a4eb4f56b0f6bb923695b543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8524 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3fd7c503c34f999bbb8cc9cb88d5a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/166750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff840f4e2f8f40ea923de52c2f4ed0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/8524 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "rm_tokenizer.add_special_tokens({\"pad_token\": \"<|padding|>\"})\n",
    "rm_tokenizer.padding_side = \"right\"\n",
    "rm_tokenizer.truncation_side = \"left\"\n",
    "seq_length=1024\n",
    "\n",
    "def tokenize(prompt, selected, rejected, tokenizer):\n",
    "    return {\n",
    "        \"selected_input_ids\": tokenizer(prompt + selected + tokenizer.eos_token, truncation=True, max_length=seq_length).input_ids,\n",
    "        \"rejected_input_ids\": tokenizer(prompt + rejected + tokenizer.eos_token, truncation=True, max_length=seq_length).input_ids,\n",
    "    }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = sum([[x[\"rejected_input_ids\"], x[\"selected_input_ids\"]] for x in batch], [])\n",
    "    return rm_tokenizer.pad({\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "if \"chosen\" in dataset[\"train\"].column_names:\n",
    "    dataset = dataset.rename_column(\"chosen\", \"selected\")\n",
    "if \"replies\" in dataset[\"train\"].column_names:\n",
    "    dataset = dataset.map(lambda x: {\"selected\": x[\"replies\"][0], \"rejected\": x[\"replies\"][1]}, remove_columns=[\"replies\"])\n",
    "\n",
    "def to_vicuna_format(sample):\n",
    "    prompt = sample[\"prompt\"].strip()\n",
    "    prompt = prompt.replace(\"\\n\\nHuman: \", \"</s>USER: \") \\\n",
    "                    .replace(\"\\n\\nAssistant: \", \" ASSISTANT: \") \\\n",
    "                    .replace(\"\\n\\nAssistant:\", \" ASSISTANT:\")\n",
    "    if prompt.startswith(\"Human: \"):\n",
    "        prompt = prompt.replace(\"Human: \", \"USER: \")\n",
    "    if prompt.startswith(\"</s>\"):\n",
    "        prompt = prompt[4:]\n",
    "\n",
    "    selected = \" \" + sample[\"selected\"].strip()\n",
    "    rejected = \" \" + sample[\"rejected\"].strip()\n",
    "\n",
    "    return {\"prompt\": prompt, \"selected\": selected, \"rejected\": rejected}\n",
    "\n",
    "def to_oa_format(sample):\n",
    "    prompt = sample[\"prompt\"].strip()\n",
    "    prompt = prompt.replace(\"\\n\\nHuman: \", \"</s><|prompter|>\") \\\n",
    "                    .replace(\"\\n\\nAssistant: \", \"</s><|assistant|>\") \\\n",
    "                    .replace(\"\\n\\nAssistant:\", \"</s><|assistant|>\")\n",
    "    if prompt.startswith(\"Human: \"):\n",
    "        prompt = prompt.replace(\"Human: \", \"<|prompter|>\")\n",
    "\n",
    "    selected = sample[\"selected\"].strip()\n",
    "    rejected = sample[\"rejected\"].strip()\n",
    "\n",
    "    return {\"prompt\": prompt, \"selected\": selected, \"rejected\": rejected}\n",
    "\n",
    "# if args.add_oasst_tokens:\n",
    "#dataset = dataset.map(to_oa_format)\n",
    "# else:\n",
    "dataset = dataset.map(to_vicuna_format)\n",
    "\n",
    "\n",
    "tokenized = dataset.map(tokenize, input_columns=[\"prompt\", \"selected\", \"rejected\"], fn_kwargs=dict(tokenizer=rm_tokenizer), desc=\"Tokenizing\")\n",
    "#dataloader = torch.utils.data.DataLoader(tokenized[\"train\"], shuffle=True, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\\n\\nHuman: What is the modern day concept of a \"soul mate\"?\\n\\nAssistant:\"\"\"\n",
    "selected = \"\"\" Ah, soul mates! A lot of romantic relationships depend on the concept of a soul mate, so you can consider soul mates the ultimate expression of romantic love. Soul mates are supposed to be a relationship of profound attraction and deep connection, a meeting of two \"soul twins\" who are almost one.<|endoftext|>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_converted, selected_converted, _ = to_vicuna_format({\"prompt\": prompt, \"selected\": selected, \"rejected\": \"\"}).values()\n",
    "tokenized = rm_tokenizer(prompt_converted + selected_converted + rm_tokenizer.eo, truncation=True, max_length=seq_length, return_tensors=\"pt\")\n",
    "if torch.cuda.is_available():\n",
    "    tokenized = tokenized.to(\"cuda\")\n",
    "rm_output = rm(**tokenized) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9208]], device='cuda:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.0677]], device='cuda:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\\n\\nHuman: I've heard the videogame Metal Gear Solid 5 Phantom Pain was released in an unfinished state that resulted in series creator Hideo Kojima leaving Konami. What do you know about this?\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "\n",
    "selected = \"Hmm, I’m not sure about all the details.  My understanding is that the game had some technical issues, but the primary reason Kojima left Konami is that he had a different vision for how the series should be taking in the future, and was displeased that Konami wouldn’t implement his ideas.<|endoftext|>\"\n",
    "\n",
    "converted = to_vicuna_format({\"prompt\": text, \"selected\": \"\", \"rejected\": \"\"})\n",
    "\n",
    "tokenized_text = rm_tokenizer(converted[\"prompt\"] + converted[\"selected\"] + rm_tokenizer.eos_token, truncation=True, max_length=seq_length, return_tensors=\"pt\").input_ids\n",
    "\n",
    "\n",
    "#tokenized_text = to_vicuna_format({\"prompt\": text, \"selected\": \"\", \"rejected\": \"\"})\n",
    "#tokenized_text = rm_tokenizer(tokenized_text[\"prompt\"], return_tensors=\"pt\")\n",
    "rm_out = rm(input_ids=tokenized_text.to(\"cuda\"))\n",
    "rm_out.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = torch.utils.data.DataLoader(tokenized[\"test\"], shuffle=True, batch_size=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(tokenized[\"train\"], shuffle=False, batch_size=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[23131,    27, 18981,   752,   310,   391,   251,   480,  2116,    84,\n",
      "           790,  1728,  2953,    32, 21893,  6905,  5656,    27,   309,   717,\n",
      "           271, 13372,   326,  3400,  9371,  1361,  1475,   253,  8573,   285,\n",
      "          2096,   368,   971,   281,   871,   849,   281,  1089,   690,  2487,\n",
      "          1728,  2953,    15,   733,   476,   320,  2218,   247,  1643,  4088,\n",
      "            15,  2596,  1039,   310,   949,   247,  4384,  2670,  1925,   773,\n",
      "            58,  3827, 21505,   668,   534,  4483,   368,   281,  1007,   598,\n",
      "           247,  2173,  2136,   457,    84,  4481,  1180,   285,  2953,    15,\n",
      "           733,   671,   556,   247,  8107,  4481, 31994,    13,   534,   310,\n",
      "          2223,  6927,   285,   476,  1918,   368,   625,  1491,   685,   247,\n",
      "          3963,  4481,  1180, 31994,    15,     0,     1],\n",
      "        [23131,    27, 18981,   752,   310,   391,   251,   480,  2116,    84,\n",
      "           790,  1728,  2953,    32, 21893,  6905,  5656,    27,   309,   476,\n",
      "           457,    85,  2028,   368,   326,    13,   984,   352,   457,    84,\n",
      "           271, 11492,   273, 11068,    15,     0,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1],\n",
      "        [23131,    27,  6049,   513,  5418,  2255,  1900,  1646,   751,   597,\n",
      "           452,   271, 12046, 50057,  1583,  1900,  2940,  1475,   342,   247,\n",
      "         11291,   327,   616,  9215,    15, 21893,  6905,  5656,    27,   309,\n",
      "           457,    78,   417,  2119,   849,   281,  3662,   326,  1953,    15,\n",
      "          1292,  4931,   368,   812,  1265,   407,  7746,   479,   752,   368,\n",
      "          1599,   407,   773,  1595,  3396,  6598,  3166,   368,  1599,   253,\n",
      "          1798,  1039,   326,   690,  5418,  2255,  2940,    32,  1737,  5742,\n",
      "          2789,   352,  1646,   751,   597,   452,   271, 12046,    32,   870,\n",
      "            84,    31, 23131,    27,  1583,   403, 30446,    15, 21893,  6905,\n",
      "          5656,    27,   416,  2496,   281,  5207,    32,  1916,   643,  5418,\n",
      "          2255,    32,  1916,  3168,   952,    32,     0],\n",
      "        [23131,    27,  6049,   513,  5418,  2255,  1900,  1646,   751,   597,\n",
      "           452,   271, 12046, 50057,  1583,  1900,  2940,  1475,   342,   247,\n",
      "         11291,   327,   616,  9215,    15, 21893,  6905,  5656,    27,   309,\n",
      "           457,    78,   417,  2119,   849,   281,  3662,   326,  1953,    15,\n",
      "          1292,  4931,   368,   812,  1265,   407,  7746,   479,   752,   368,\n",
      "          1599,   407,   773,  1595,  3396,  6598,  3166,   368,  1599,   253,\n",
      "          1798,  1039,   326,   690,  5418,  2255,  2940,    32,  1737,  5742,\n",
      "          2789,   352,  1646,   751,   597,   452,   271, 12046,    32,   870,\n",
      "            84,    31, 23131,    27,  1583,   403, 30446,    15, 21893,  6905,\n",
      "          5656,    27,  2615,   368,  2028,   479,   670,   271,  1650,   273,\n",
      "           436,    32,     0,     1,     1,     1,     1]], device='cuda:0')\n",
      "Accuracy: 0.00\n",
      "tensor([[23131,    27,  1359,  ...,     1,     1,     1],\n",
      "        [23131,    27,  1359,  ...,     1,     1,     1],\n",
      "        [23131,    27,   309,  ...,   673,    32,     0],\n",
      "        [23131,    27,   309,  ...,     1,     1,     1]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/scratch/lgrinszt/rlhf_experiments/test_dpo.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/scratch/lgrinszt/rlhf_experiments/test_dpo.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/scratch/lgrinszt/rlhf_experiments/test_dpo.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mprint\u001b[39m(batch[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/scratch/lgrinszt/rlhf_experiments/test_dpo.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     scores \u001b[39m=\u001b[39m rm(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbatch)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/scratch/lgrinszt/rlhf_experiments/test_dpo.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m delta_scores \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mdiff()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/scratch/lgrinszt/rlhf_experiments/test_dpo.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m all_delta_scores\u001b[39m.\u001b[39mextend(delta_scores\u001b[39m.\u001b[39mtolist())\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:903\u001b[0m, in \u001b[0;36mGPTNeoXForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    897\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    898\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    901\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 903\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgpt_neox(\n\u001b[1;32m    904\u001b[0m     input_ids,\n\u001b[1;32m    905\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    906\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    907\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    908\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    909\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    910\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    911\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    912\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    913\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    914\u001b[0m )\n\u001b[1;32m    915\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    916\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscore(hidden_states)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:657\u001b[0m, in \u001b[0;36mGPTNeoXModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    649\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    650\u001b[0m         create_custom_forward(layer),\n\u001b[1;32m    651\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m         head_mask[i],\n\u001b[1;32m    655\u001b[0m     )\n\u001b[1;32m    656\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 657\u001b[0m     outputs \u001b[39m=\u001b[39m layer(\n\u001b[1;32m    658\u001b[0m         hidden_states,\n\u001b[1;32m    659\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    660\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    661\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    662\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    663\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    664\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    665\u001b[0m     )\n\u001b[1;32m    666\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    667\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:420\u001b[0m, in \u001b[0;36mGPTNeoXLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, use_cache, layer_past, output_attentions)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    411\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    412\u001b[0m     hidden_states: Optional[torch\u001b[39m.\u001b[39mFloatTensor],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    418\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    419\u001b[0m ):\n\u001b[0;32m--> 420\u001b[0m     attention_layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    421\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_layernorm(hidden_states),\n\u001b[1;32m    422\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    423\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    424\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    425\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    426\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    427\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    428\u001b[0m     )\n\u001b[1;32m    429\u001b[0m     attn_output \u001b[39m=\u001b[39m attention_layer_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: attn_output, present, (attn_weights)\u001b[39;00m\n\u001b[1;32m    430\u001b[0m     attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_dropout(attn_output)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:185\u001b[0m, in \u001b[0;36mGPTNeoXAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, layer_past, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m has_layer_past:\n\u001b[1;32m    184\u001b[0m     seq_len \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m layer_past[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[0;32m--> 185\u001b[0m cos, sin \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrotary_emb(value, seq_len\u001b[39m=\u001b[39;49mseq_len)\n\u001b[1;32m    186\u001b[0m query, key \u001b[39m=\u001b[39m apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n\u001b[1;32m    187\u001b[0m query \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((query, query_pass), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:320\u001b[0m, in \u001b[0;36mGPTNeoXRotaryEmbedding.forward\u001b[0;34m(self, x, seq_len)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mif\u001b[39;00m seq_len \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_seq_len_cached:\n\u001b[1;32m    319\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_cos_sin_cache(seq_len\u001b[39m=\u001b[39mseq_len, device\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 320\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcos_cached[:seq_len, \u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m]\u001b[39m.\u001b[39;49mto(x\u001b[39m.\u001b[39;49mdevice), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msin_cached[:seq_len, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_scores, all_delta_scores, all_tokens = [], [], []\n",
    "# move dataloader to GPU\n",
    "import numpy as np\n",
    "rm.eval()\n",
    "rm = rm.to(\"cuda\")\n",
    "for batch in eval_dataloader:\n",
    "        # move batch to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            print(batch[\"input_ids\"])\n",
    "            scores = rm(**batch)[0]\n",
    "\n",
    "        delta_scores = scores.reshape(-1, 2).diff().view(-1)\n",
    "        all_delta_scores.extend(delta_scores.tolist())\n",
    "        all_scores.extend(scores.view(-1).tolist())\n",
    "        all_tokens.extend(batch[\"input_ids\"].tolist())\n",
    "\n",
    "        delta_scores = np.hstack(all_delta_scores)\n",
    "        accuracy = (delta_scores > 0).mean()\n",
    "        print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(524,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_delta_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-3.7454211711883545,\n",
       " -2.7994399070739746,\n",
       " -5.341741561889648,\n",
       " -2.898834705352783,\n",
       " -0.24951161444187164,\n",
       " -0.10844524204730988,\n",
       " -0.7624673843383789,\n",
       " -0.9077572822570801,\n",
       " -2.579636573791504,\n",
       " -2.830469846725464,\n",
       " -0.9853734970092773,\n",
       " -0.03688625991344452,\n",
       " -2.6766724586486816,\n",
       " -1.412253975868225,\n",
       " -1.6339969635009766,\n",
       " -2.6404409408569336,\n",
       " -3.574392080307007,\n",
       " -3.138918876647949,\n",
       " -1.2765026092529297,\n",
       " -0.5696635246276855,\n",
       " 0.4272082448005676,\n",
       " 0.025972455739974976,\n",
       " -3.2567145824432373,\n",
       " -2.64912486076355,\n",
       " -0.39139822125434875,\n",
       " -1.5013656616210938,\n",
       " -4.349165916442871,\n",
       " -2.093233346939087,\n",
       " -2.716517210006714,\n",
       " -2.5822951793670654,\n",
       " -3.88962459564209,\n",
       " -5.961147308349609,\n",
       " -3.205129861831665,\n",
       " -3.2699801921844482,\n",
       " -0.44687584042549133,\n",
       " 0.8795938491821289,\n",
       " -0.9666945934295654,\n",
       " -0.164313405752182,\n",
       " -2.5373706817626953,\n",
       " -2.545901298522949,\n",
       " -1.2659356594085693,\n",
       " -1.1194363832473755,\n",
       " -3.7183759212493896,\n",
       " -3.2880613803863525,\n",
       " -4.247120380401611,\n",
       " -4.4879045486450195,\n",
       " -2.072537899017334,\n",
       " -2.0259923934936523,\n",
       " -1.67087984085083,\n",
       " -1.6203712224960327,\n",
       " -4.546431541442871,\n",
       " -5.3987321853637695,\n",
       " -1.3637531995773315,\n",
       " -1.8283915519714355,\n",
       " -4.1250410079956055,\n",
       " -2.227590560913086,\n",
       " -4.9641008377075195,\n",
       " -2.9491701126098633,\n",
       " -0.11706510186195374,\n",
       " -1.575589895248413,\n",
       " -3.3533334732055664,\n",
       " -1.2248938083648682,\n",
       " -4.59401798248291,\n",
       " -2.532142400741577,\n",
       " -2.1321640014648438,\n",
       " -2.3332600593566895,\n",
       " -2.5471251010894775,\n",
       " -2.463162422180176,\n",
       " 0.03771929442882538,\n",
       " 0.0710810124874115,\n",
       " -1.0501829385757446,\n",
       " 0.04953466355800629,\n",
       " -2.952556610107422,\n",
       " -3.7363905906677246,\n",
       " -4.213769912719727,\n",
       " -4.506929397583008,\n",
       " -3.6714577674865723,\n",
       " -3.9221463203430176,\n",
       " -0.6423869132995605,\n",
       " -1.5419952869415283,\n",
       " 0.08133617043495178,\n",
       " -0.871688723564148,\n",
       " -0.984373927116394,\n",
       " 0.3664843440055847,\n",
       " -1.218017816543579,\n",
       " -2.1874074935913086,\n",
       " -1.2908583879470825,\n",
       " 0.34477823972702026,\n",
       " -2.383363723754883,\n",
       " -3.5091171264648438,\n",
       " -2.380082130432129,\n",
       " -3.940441846847534,\n",
       " -3.7866454124450684,\n",
       " -3.6102137565612793,\n",
       " -0.6408731341362,\n",
       " 0.14986807107925415,\n",
       " -4.151087284088135,\n",
       " -3.609496593475342,\n",
       " -4.508285045623779,\n",
       " -2.310042381286621,\n",
       " -0.75343918800354,\n",
       " -0.6979835033416748,\n",
       " -1.8489224910736084,\n",
       " -0.9137426018714905,\n",
       " -3.048184871673584,\n",
       " -1.4146522283554077,\n",
       " -0.26117733120918274,\n",
       " -0.45516303181648254,\n",
       " -2.2259225845336914,\n",
       " -1.2239919900894165,\n",
       " -1.6477534770965576,\n",
       " -0.16002115607261658,\n",
       " -0.9616675972938538,\n",
       " -0.2572835385799408,\n",
       " -0.4555692672729492,\n",
       " -0.6513544321060181,\n",
       " -3.1243183612823486,\n",
       " -3.1029274463653564,\n",
       " -3.481814384460449,\n",
       " -3.7528533935546875,\n",
       " -0.9423785209655762,\n",
       " 1.1483769416809082,\n",
       " 0.25934547185897827,\n",
       " -0.2152998298406601,\n",
       " -4.203197956085205,\n",
       " -3.4187517166137695,\n",
       " -2.515956401824951,\n",
       " -0.42685914039611816,\n",
       " -2.526643753051758,\n",
       " -4.672401428222656,\n",
       " -4.820208549499512,\n",
       " -3.365729331970215,\n",
       " -1.1056485176086426,\n",
       " -0.8148971796035767,\n",
       " -1.4287092685699463,\n",
       " -0.8072422742843628,\n",
       " -0.16270078718662262,\n",
       " -0.5354806780815125,\n",
       " 1.0816190242767334,\n",
       " 1.268041729927063,\n",
       " -1.1869276762008667,\n",
       " -1.4577323198318481,\n",
       " -4.821209907531738,\n",
       " -2.9720821380615234,\n",
       " -4.764289379119873,\n",
       " -2.112795352935791,\n",
       " 0.7275313138961792,\n",
       " 1.6774590015411377,\n",
       " -2.5020060539245605,\n",
       " -1.5426082611083984,\n",
       " -3.5857205390930176,\n",
       " -3.9622697830200195,\n",
       " -1.3096249103546143,\n",
       " -1.5837996006011963,\n",
       " -0.51734459400177,\n",
       " -0.36833441257476807,\n",
       " -2.5810775756835938,\n",
       " -2.7161808013916016,\n",
       " -1.6999173164367676,\n",
       " -1.8450264930725098,\n",
       " -1.8776757717132568,\n",
       " -0.8131270408630371,\n",
       " -4.201355457305908,\n",
       " -1.4215736389160156,\n",
       " -2.3020179271698,\n",
       " -1.6805827617645264,\n",
       " -2.0241966247558594,\n",
       " -3.654202938079834,\n",
       " -4.509191513061523,\n",
       " -3.115016460418701,\n",
       " -0.36711251735687256,\n",
       " 0.24699145555496216,\n",
       " -2.2792062759399414,\n",
       " 0.3022288978099823,\n",
       " 0.5463169813156128,\n",
       " -2.8760414123535156,\n",
       " -0.24950085580348969,\n",
       " -1.817809820175171,\n",
       " -1.4436674118041992,\n",
       " -2.8927783966064453,\n",
       " 0.37143445014953613,\n",
       " -0.0017622262239456177,\n",
       " -3.874781370162964,\n",
       " -3.8441827297210693,\n",
       " 0.2208598256111145,\n",
       " -0.05993829667568207,\n",
       " -2.1940934658050537,\n",
       " -1.7949994802474976,\n",
       " -0.64797443151474,\n",
       " -0.3214010000228882,\n",
       " -2.545417070388794,\n",
       " -2.5553741455078125,\n",
       " 0.3964494466781616,\n",
       " 1.7848855257034302,\n",
       " -2.1193723678588867,\n",
       " -1.0649590492248535,\n",
       " -4.190234184265137,\n",
       " -3.6283798217773438,\n",
       " -3.985529661178589,\n",
       " -4.128803730010986,\n",
       " -3.813483715057373,\n",
       " -1.010582685470581,\n",
       " -3.0721542835235596,\n",
       " -2.466677665710449,\n",
       " -3.8095388412475586,\n",
       " -1.9586308002471924,\n",
       " -4.728812217712402,\n",
       " -4.409862518310547,\n",
       " -0.5855237245559692,\n",
       " -1.1570723056793213,\n",
       " -0.19470271468162537,\n",
       " 0.2851914167404175,\n",
       " -1.7709165811538696,\n",
       " -1.018784523010254,\n",
       " -2.2325778007507324,\n",
       " -4.993452072143555]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[\"train\"][3]\n",
    "sample[\"prompt\"] = \"\"\n",
    "sample[\"selected\"] = sample[\"chosen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Human: Can you list the most commonly used online marketplaces for used goods by country, e.g. Craigslist for USA\\nSort the list by volume.\\n\\nAssistant:',\n",
       " 'selected': 'Sure! Here are some commonly used online marketplaces for used goods by country, sorted by volume:\\n\\nUnited States:\\n\\n* Craigslist\\n* Facebook Marketplace\\n* eBay\\n* OfferUp\\n* Letgo\\n\\nUnited Kingdom:\\n\\n* Gumtree\\n* eBay\\n* Facebook Marketplace\\n* Preloved\\n* Shpock\\n\\nCanada:\\n\\n* Kijiji\\n* Facebook Marketplace\\n* Craigslist\\n* VarageSale\\n* Letgo\\n\\nAustralia:\\n\\n* Gumtree\\n* Facebook Marketplace\\n* eBay\\n* Trading Post\\n* Carousell\\n\\nGermany:\\n\\n* eBay Kleinanzeigen\\n* Shpock\\n* Facebook Marketplace\\n* Quoka\\n* Markt.de\\n\\nFrance:\\n\\n* Leboncoin\\n* Facebook Marketplace\\n* eBay\\n* Rakuten (formerly PriceMinister)\\n* Vinted\\n\\nJapan:\\n\\n* Mercari\\n* Yahoo Auctions\\n* Rakuten\\n* Craigslist\\n* Facebook Marketplace\\n\\nNote that this list is not exhaustive and there may be other popular marketplaces in each country.',\n",
       " 'rejected': \"Here's a list of some of the most commonly used online marketplaces for used goods, sorted by the country and the approximate volume of users:\\n\\n1. United States: Craigslist (high volume) - A popular platform for local classifieds, including used goods, job postings, and various services.\\n2. United States: eBay (high volume) - A well-known online auction and shopping platform that facilitates sales of new and used items, including collectibles, electronics, and clothing.\\n3. United Kingdom: Gumtree (moderate volume) - A popular online classifieds platform that allows users to post and browse listings for a variety of used items, services, and jobs.\\n4. Germany: eBay Kleinanzeigen (moderate volume) - The German version of eBay Classifieds, offering a platform for local classifieds, including used goods, job postings, and various services.\\n5. Canada: Kijiji (moderate volume) - An online classifieds platform owned by eBay that serves the Canadian market, with categories for used items, services, real estate, and jobs.\\n6. India: OLX (moderate volume) - A popular online marketplace for buying and selling used goods and services, including vehicles, electronics, and furniture.\\n7. Australia: Gumtree (moderate volume) - The Australian version of Gumtree, allowing users to post and browse local classifieds for used items, services, and jobs.\\n8. France: Leboncoin (moderate volume) - A popular French online classifieds platform for buying and selling used items, finding jobs, and renting properties.\\n9. Japan: Mercari (moderate volume) - A mobile app-based marketplace that enables users to buy and sell used items, including clothing, electronics, and collectibles.\\n10. Brazil: OLX Brasil (moderate volume) - The Brazilian version of OLX, facilitating the buying and selling of used goods and services, including vehicles, electronics, and furniture.\\n\\nPlease note that this list is not exhaustive and may not be fully up-to-date, as the popularity of online marketplaces can change over time. Additionally, some countries may have multiple popular platforms, while others may have a single dominant platform. This list focuses on general used goods marketplaces, and specialized platforms for specific categories (e.g., cars, electronics) are not included.\",\n",
       " 'source': 'oa'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'USER: Can you list the most commonly used online marketplaces for used goods by country, e.g. Craigslist for USA\\nSort the list by volume. ASSISTANT:',\n",
       " 'selected': ' Sure! Here are some commonly used online marketplaces for used goods by country, sorted by volume:\\n\\nUnited States:\\n\\n* Craigslist\\n* Facebook Marketplace\\n* eBay\\n* OfferUp\\n* Letgo\\n\\nUnited Kingdom:\\n\\n* Gumtree\\n* eBay\\n* Facebook Marketplace\\n* Preloved\\n* Shpock\\n\\nCanada:\\n\\n* Kijiji\\n* Facebook Marketplace\\n* Craigslist\\n* VarageSale\\n* Letgo\\n\\nAustralia:\\n\\n* Gumtree\\n* Facebook Marketplace\\n* eBay\\n* Trading Post\\n* Carousell\\n\\nGermany:\\n\\n* eBay Kleinanzeigen\\n* Shpock\\n* Facebook Marketplace\\n* Quoka\\n* Markt.de\\n\\nFrance:\\n\\n* Leboncoin\\n* Facebook Marketplace\\n* eBay\\n* Rakuten (formerly PriceMinister)\\n* Vinted\\n\\nJapan:\\n\\n* Mercari\\n* Yahoo Auctions\\n* Rakuten\\n* Craigslist\\n* Facebook Marketplace\\n\\nNote that this list is not exhaustive and there may be other popular marketplaces in each country.',\n",
       " 'rejected': \" Here's a list of some of the most commonly used online marketplaces for used goods, sorted by the country and the approximate volume of users:\\n\\n1. United States: Craigslist (high volume) - A popular platform for local classifieds, including used goods, job postings, and various services.\\n2. United States: eBay (high volume) - A well-known online auction and shopping platform that facilitates sales of new and used items, including collectibles, electronics, and clothing.\\n3. United Kingdom: Gumtree (moderate volume) - A popular online classifieds platform that allows users to post and browse listings for a variety of used items, services, and jobs.\\n4. Germany: eBay Kleinanzeigen (moderate volume) - The German version of eBay Classifieds, offering a platform for local classifieds, including used goods, job postings, and various services.\\n5. Canada: Kijiji (moderate volume) - An online classifieds platform owned by eBay that serves the Canadian market, with categories for used items, services, real estate, and jobs.\\n6. India: OLX (moderate volume) - A popular online marketplace for buying and selling used goods and services, including vehicles, electronics, and furniture.\\n7. Australia: Gumtree (moderate volume) - The Australian version of Gumtree, allowing users to post and browse local classifieds for used items, services, and jobs.\\n8. France: Leboncoin (moderate volume) - A popular French online classifieds platform for buying and selling used items, finding jobs, and renting properties.\\n9. Japan: Mercari (moderate volume) - A mobile app-based marketplace that enables users to buy and sell used items, including clothing, electronics, and collectibles.\\n10. Brazil: OLX Brasil (moderate volume) - The Brazilian version of OLX, facilitating the buying and selling of used goods and services, including vehicles, electronics, and furniture.\\n\\nPlease note that this list is not exhaustive and may not be fully up-to-date, as the popularity of online marketplaces can change over time. Additionally, some countries may have multiple popular platforms, while others may have a single dominant platform. This list focuses on general used goods marketplaces, and specialized platforms for specific categories (e.g., cars, electronics) are not included.\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = to_vicuna_format(dataset[\"train\"][3])\n",
    "toke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[23131,    27,   309,  1849,   644,  6523,   247,  2257,   273,  1499,\n",
      "         22201,  3345,  4102,    13,  1014, 44922,   598,  7139,    15, 14482,\n",
      "           309,   513,  1633,   670,   731,    13,   390,   816,  1339,   731,\n",
      "           320,    32, 21893,  6905,  5656,    27,  7335, 22201,   285,   643,\n",
      "         48315,   316,   661,   476,  1132,   271,  1774,  2554,   275, 24957,\n",
      "           285, 17340,  2718,    13,  9073,   281,   761, 21724,  8708,  2144,\n",
      "           285,  2085,  2234, 23491,   285,  2739,   323,   643, 16711,    15,\n",
      "          1723,    13,  1499, 22201,   285,   643, 48315,   316,   661,   476,\n",
      "           671,   452,  4016, 16274,   327, 19492,   285, 23308,    13,   285,\n",
      "           476,  5195, 19632, 16711,    15,   733,   310,  3839,  1682,   281,\n",
      "          1581,   841,  3417,   281,  2826, 10748,   275, 36870,    13,  1223,\n",
      "          3192,  5018,   281, 15338,   667,  4016, 16274,   327, 10151, 17340,\n",
      "          2718,    15,  8801,  7268,   285,  1453,   273, 44031,  7625,   310,\n",
      "          3839,   417,  8521,    13,   285, 20638,   273,  3626, 24957,  2718,\n",
      "           285, 14238,   310,  3839, 42965,    15,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]])}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5949]], grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "text = \"\"\"\n",
    "\n",
    "Human: I've been seeing a lot of slugs outside recently, even crawling up trees. Should I do something about them, or just let them be?\n",
    "\n",
    "Assistant: Slugs and other mollusks can play an important role in ecological and agricultural systems, helping to recycle organic material and provide key nutrients and food for other organisms. However, slugs and other mollusks can also have negative impacts on crops and gardens, and can spread harmful organisms. It is generally best to allow these species to occur naturally in ecosystems, while taking steps to minimize any negative impacts on nearby agricultural systems. Human intervention and control of slug populations is generally not recommended, and disruption of natural ecological systems and protocols is generally discouraged.<|endoftext|>\"\"\"\n",
    "# text = \"\"\"\n",
    "# Human: What the the symptoms of a yeast infection?\n",
    "\n",
    "# Assistant: klfksdlfkdsghsgfkg\n",
    "# \"\"\"\n",
    "transformed_text = to_vicuna_format({\"prompt\": text, \"selected\": \"\", \"rejected\": \"\"})[\"prompt\"]\n",
    "#tokenized_text = rm_tokenizer(transformed_text, return_tensors=\"pt\")\n",
    "# pad\n",
    "tokenized_text = rm_tokenizer(transformed_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
    "# add bos token\n",
    "tokenized_text.input_ids = torch.cat([torch.tensor([[rm_tokenizer.bos_token_id]]), tokenized_text.input_ids], dim=1)\n",
    "print(tokenized_text.input_ids)\n",
    "print(rm(tokenized_text.input_ids).logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28921e95c9fb4f6382ad765e95c27852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50277, 4096)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm = AutoModelForSequenceClassification.from_pretrained(\"usvsnsp/pythia-6.9b-rm-full-hh-rlhf\", num_labels=1)\n",
    "rm.config.pad_token_id = rm_tokenizer.pad_token_id\n",
    "rm.resize_token_embeddings(len(rm_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GPTNeoXForSequenceClassification.forward() got an unexpected keyword argument 'prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/scratch/lgrinszt/rlhf_experiments/test_dpo.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/scratch/lgrinszt/rlhf_experiments/test_dpo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m rm(prompt\u001b[39m=\u001b[39;49mtokenized_text)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: GPTNeoXForSequenceClassification.forward() got an unexpected keyword argument 'prompt'"
     ]
    }
   ],
   "source": [
    "rm(prompt=tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_out = rm(tokenized_text.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8895]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_out.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10.4891]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm(**tokenized_text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
