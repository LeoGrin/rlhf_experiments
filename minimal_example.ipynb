{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download reward model\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "rm_tokenizer = AutoTokenizer.from_pretrained(\"usvsnsp/pythia-6.9b-rm-full-hh-rlhf\")\n",
    "rm = AutoModelForSequenceClassification.from_pretrained(\"usvsnsp/pythia-6.9b-rm-full-hh-rlhf\")\n",
    "seq_length = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try on a single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\\n\\nHuman: What is the modern day concept of a \"soul mate\"?\\n\\nAssistant:\"\"\"\n",
    "selected = \"\"\" Ah, soul mates! A lot of romantic relationships depend on the concept of a soul mate, so you can consider soul mates the ultimate expression of romantic love. Soul mates are supposed to be a relationship of profound attraction and deep connection, a meeting of two \"soul twins\" who are almost one.\"\"\"\n",
    "#prompt_converted, selected_converted, _ = to_vicuna_format({\"prompt\": prompt, \"selected\": selected, \"rejected\": \"\"}).values()\n",
    "tokenized = rm_tokenizer(rm_tokenizer.bos_token + prompt + selected + rm_tokenizer.eos_token, truncation=True, max_length=seq_length, return_tensors=\"pt\")\n",
    "if torch.cuda.is_available():\n",
    "    tokenized = tokenized.to(\"cuda\")\n",
    "rm_output = rm(**tokenized) \n",
    "rm_output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#dataset = load_dataset(\"Anthropic/hh-rlhf\")\n",
    "dataset = load_dataset(\"pvduy/rm_oa_hh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "rm_tokenizer.add_special_tokens({\"pad_token\": \"<|padding|>\"})\n",
    "rm_tokenizer.padding_side = \"right\"\n",
    "rm_tokenizer.truncation_side = \"left\"\n",
    "seq_length=1024\n",
    "\n",
    "def tokenize(prompt, selected, rejected, tokenizer):\n",
    "    return {\n",
    "        \"selected_input_ids\": tokenizer(prompt + selected + tokenizer.eos_token, truncation=True, max_length=seq_length).input_ids,\n",
    "        \"rejected_input_ids\": tokenizer(prompt + rejected + tokenizer.eos_token, truncation=True, max_length=seq_length).input_ids,\n",
    "    }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = sum([[x[\"rejected_input_ids\"], x[\"selected_input_ids\"]] for x in batch], [])\n",
    "    return rm_tokenizer.pad({\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "if \"chosen\" in dataset[\"train\"].column_names:\n",
    "    dataset = dataset.rename_column(\"chosen\", \"selected\")\n",
    "if \"replies\" in dataset[\"train\"].column_names:\n",
    "    dataset = dataset.map(lambda x: {\"selected\": x[\"replies\"][0], \"rejected\": x[\"replies\"][1]}, remove_columns=[\"replies\"])\n",
    "\n",
    "def to_vicuna_format(sample):\n",
    "    prompt = sample[\"prompt\"].strip()\n",
    "    prompt = prompt.replace(\"\\n\\nHuman: \", \"</s>USER: \") \\\n",
    "                    .replace(\"\\n\\nAssistant: \", \" ASSISTANT: \") \\\n",
    "                    .replace(\"\\n\\nAssistant:\", \" ASSISTANT:\")\n",
    "    if prompt.startswith(\"Human: \"):\n",
    "        prompt = prompt.replace(\"Human: \", \"USER: \")\n",
    "    if prompt.startswith(\"</s>\"):\n",
    "        prompt = prompt[4:]\n",
    "\n",
    "    selected = \" \" + sample[\"selected\"].strip()\n",
    "    rejected = \" \" + sample[\"rejected\"].strip()\n",
    "\n",
    "    return {\"prompt\": prompt, \"selected\": selected, \"rejected\": rejected}\n",
    "\n",
    "def to_oa_format(sample):\n",
    "    prompt = sample[\"prompt\"].strip()\n",
    "    prompt = prompt.replace(\"\\n\\nHuman: \", \"</s><|prompter|>\") \\\n",
    "                    .replace(\"\\n\\nAssistant: \", \"</s><|assistant|>\") \\\n",
    "                    .replace(\"\\n\\nAssistant:\", \"</s><|assistant|>\")\n",
    "    if prompt.startswith(\"Human: \"):\n",
    "        prompt = prompt.replace(\"Human: \", \"<|prompter|>\")\n",
    "\n",
    "    selected = sample[\"selected\"].strip()\n",
    "    rejected = sample[\"rejected\"].strip()\n",
    "\n",
    "    return {\"prompt\": prompt, \"selected\": selected, \"rejected\": rejected}\n",
    "\n",
    "# if args.add_oasst_tokens:\n",
    "#dataset = dataset.map(to_oa_format)\n",
    "# else:\n",
    "dataset = dataset.map(to_vicuna_format)\n",
    "\n",
    "\n",
    "tokenized = dataset.map(tokenize, input_columns=[\"prompt\", \"selected\", \"rejected\"], fn_kwargs=dict(tokenizer=rm_tokenizer), desc=\"Tokenizing\")\n",
    "#dataloader = torch.utils.data.DataLoader(tokenized[\"train\"], shuffle=True, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = torch.utils.data.DataLoader(tokenized[\"test\"], shuffle=True, batch_size=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores, all_delta_scores, all_tokens = [], [], []\n",
    "# move dataloader to GPU\n",
    "import numpy as np\n",
    "rm.eval()\n",
    "rm = rm.to(\"cuda\")\n",
    "for batch in eval_dataloader:\n",
    "        # move batch to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            print(batch[\"input_ids\"])\n",
    "            scores = rm(**batch)[0]\n",
    "\n",
    "        delta_scores = scores.reshape(-1, 2).diff().view(-1)\n",
    "        all_delta_scores.extend(delta_scores.tolist())\n",
    "        all_scores.extend(scores.view(-1).tolist())\n",
    "        all_tokens.extend(batch[\"input_ids\"].tolist())\n",
    "\n",
    "        delta_scores = np.hstack(all_delta_scores)\n",
    "        accuracy = (delta_scores > 0).mean()\n",
    "        print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
