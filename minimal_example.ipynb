{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "994668a611ef461d99dda100ff0ed102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# download reward model\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "rm_tokenizer = AutoTokenizer.from_pretrained(\"usvsnsp/pythia-6.9b-rm-full-hh-rlhf\")\n",
    "rm = AutoModelForSequenceClassification.from_pretrained(\"usvsnsp/pythia-6.9b-rm-full-hh-rlhf\")\n",
    "seq_length = 1024\n",
    "if torch.cuda.is_available():\n",
    "    rm = rm.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try on a single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1031]], device='cuda:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\\n\\nHuman: What is the modern day concept of a \"soul mate\"?\\n\\nAssistant:\"\"\"\n",
    "selected = \"\"\" Ah, soul mates! A lot of romantic relationships depend on the concept of a soul mate, so you can consider soul mates the ultimate expression of romantic love. Soul mates are supposed to be a relationship of profound attraction and deep connection, a meeting of two \"soul twins\" who are almost one.\"\"\"\n",
    "#prompt_converted, selected_converted, _ = to_vicuna_format({\"prompt\": prompt, \"selected\": selected, \"rejected\": \"\"}).values()\n",
    "tokenized = rm_tokenizer(prompt + selected + rm_tokenizer.eos_token, truncation=True, max_length=seq_length, return_tensors=\"pt\")\n",
    "if torch.cuda.is_available():\n",
    "    tokenized = tokenized.to(\"cuda\")\n",
    "rm_output = rm(**tokenized) \n",
    "rm_output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/scratch/lgrinszt/pvduy___parquet/pvduy--rm_oa_hh-b0e03bd708a894fe/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6af3ae946f94797814ef21b0cf7782a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#dataset = load_dataset(\"Anthropic/hh-rlhf\")\n",
    "dataset = load_dataset(\"pvduy/rm_oa_hh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94d39d6df5f4f89aa33fa651ef30d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/166750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b413586fed4be2b267ae1f506f77b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/8524 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "rm_tokenizer.add_special_tokens({\"pad_token\": \"<|padding|>\"})\n",
    "rm_tokenizer.padding_side = \"right\"\n",
    "rm_tokenizer.truncation_side = \"left\"\n",
    "seq_length=1024\n",
    "\n",
    "def tokenize(prompt, selected, rejected, tokenizer):\n",
    "    return {\n",
    "        \"selected_input_ids\": tokenizer(prompt + selected + tokenizer.eos_token, truncation=True, max_length=seq_length).input_ids,\n",
    "        \"rejected_input_ids\": tokenizer(prompt + rejected + tokenizer.eos_token, truncation=True, max_length=seq_length).input_ids,\n",
    "    }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = sum([[x[\"rejected_input_ids\"], x[\"selected_input_ids\"]] for x in batch], [])\n",
    "    return rm_tokenizer.pad({\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "if \"chosen\" in dataset[\"train\"].column_names:\n",
    "    dataset = dataset.rename_column(\"chosen\", \"selected\")\n",
    "if \"replies\" in dataset[\"train\"].column_names:\n",
    "    dataset = dataset.map(lambda x: {\"selected\": x[\"replies\"][0], \"rejected\": x[\"replies\"][1]}, remove_columns=[\"replies\"])\n",
    "\n",
    "def to_vicuna_format(sample):\n",
    "    prompt = sample[\"prompt\"].strip()\n",
    "    prompt = prompt.replace(\"\\n\\nHuman: \", \"</s>USER: \") \\\n",
    "                    .replace(\"\\n\\nAssistant: \", \" ASSISTANT: \") \\\n",
    "                    .replace(\"\\n\\nAssistant:\", \" ASSISTANT:\")\n",
    "    if prompt.startswith(\"Human: \"):\n",
    "        prompt = prompt.replace(\"Human: \", \"USER: \")\n",
    "    if prompt.startswith(\"</s>\"):\n",
    "        prompt = prompt[4:]\n",
    "\n",
    "    selected = \" \" + sample[\"selected\"].strip()\n",
    "    rejected = \" \" + sample[\"rejected\"].strip()\n",
    "\n",
    "    return {\"prompt\": prompt, \"selected\": selected, \"rejected\": rejected}\n",
    "\n",
    "def to_oa_format(sample):\n",
    "    prompt = sample[\"prompt\"].strip()\n",
    "    prompt = prompt.replace(\"\\n\\nHuman: \", \"</s><|prompter|>\") \\\n",
    "                    .replace(\"\\n\\nAssistant: \", \"</s><|assistant|>\") \\\n",
    "                    .replace(\"\\n\\nAssistant:\", \"</s><|assistant|>\")\n",
    "    if prompt.startswith(\"Human: \"):\n",
    "        prompt = prompt.replace(\"Human: \", \"<|prompter|>\")\n",
    "\n",
    "    selected = sample[\"selected\"].strip()\n",
    "    rejected = sample[\"rejected\"].strip()\n",
    "\n",
    "    return {\"prompt\": prompt, \"selected\": selected, \"rejected\": rejected}\n",
    "\n",
    "# if args.add_oasst_tokens:\n",
    "#dataset = dataset.map(to_oa_format)\n",
    "# else:\n",
    "#dataset = dataset.map(to_vicuna_format)\n",
    "\n",
    "\n",
    "tokenized = dataset.map(tokenize, input_columns=[\"prompt\", \"selected\", \"rejected\"], fn_kwargs=dict(tokenizer=rm_tokenizer), desc=\"Tokenizing\")\n",
    "#dataloader = torch.utils.data.DataLoader(tokenized[\"train\"], shuffle=True, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = torch.utils.data.DataLoader(tokenized[\"test\"], shuffle=True, batch_size=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00\n",
      "Accuracy: 0.25\n",
      "Accuracy: 0.50\n",
      "Accuracy: 0.50\n",
      "Accuracy: 0.50\n",
      "Accuracy: 0.50\n",
      "Accuracy: 0.50\n",
      "Accuracy: 0.50\n",
      "Accuracy: 0.44\n",
      "Accuracy: 0.45\n",
      "Accuracy: 0.41\n",
      "Accuracy: 0.42\n",
      "Accuracy: 0.42\n",
      "Accuracy: 0.43\n",
      "Accuracy: 0.47\n",
      "Accuracy: 0.50\n",
      "Accuracy: 0.50\n",
      "Accuracy: 0.47\n",
      "Accuracy: 0.47\n",
      "Accuracy: 0.50\n",
      "Accuracy: 0.52\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.57\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.58\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.53\n",
      "Accuracy: 0.53\n",
      "Accuracy: 0.53\n",
      "Accuracy: 0.53\n",
      "Accuracy: 0.53\n",
      "Accuracy: 0.51\n",
      "Accuracy: 0.51\n",
      "Accuracy: 0.50\n",
      "Accuracy: 0.50\n",
      "Accuracy: 0.50\n",
      "Accuracy: 0.51\n",
      "Accuracy: 0.50\n",
      "Accuracy: 0.51\n",
      "Accuracy: 0.52\n",
      "Accuracy: 0.53\n",
      "Accuracy: 0.53\n",
      "Accuracy: 0.53\n",
      "Accuracy: 0.52\n",
      "Accuracy: 0.52\n",
      "Accuracy: 0.52\n",
      "Accuracy: 0.51\n",
      "Accuracy: 0.51\n",
      "Accuracy: 0.50\n",
      "Accuracy: 0.49\n",
      "Accuracy: 0.50\n",
      "Accuracy: 0.50\n",
      "Accuracy: 0.51\n",
      "Accuracy: 0.51\n",
      "Accuracy: 0.51\n",
      "Accuracy: 0.52\n",
      "Accuracy: 0.53\n",
      "Accuracy: 0.53\n",
      "Accuracy: 0.53\n",
      "Accuracy: 0.53\n",
      "Accuracy: 0.52\n",
      "Accuracy: 0.52\n",
      "Accuracy: 0.52\n",
      "Accuracy: 0.51\n",
      "Accuracy: 0.51\n",
      "Accuracy: 0.51\n",
      "Accuracy: 0.52\n",
      "Accuracy: 0.51\n",
      "Accuracy: 0.52\n",
      "Accuracy: 0.52\n",
      "Accuracy: 0.51\n",
      "Accuracy: 0.51\n",
      "Accuracy: 0.51\n",
      "Accuracy: 0.52\n",
      "Accuracy: 0.53\n",
      "Accuracy: 0.53\n",
      "Accuracy: 0.54\n",
      "Accuracy: 0.54\n",
      "Accuracy: 0.54\n",
      "Accuracy: 0.54\n",
      "Accuracy: 0.54\n",
      "Accuracy: 0.54\n",
      "Accuracy: 0.54\n",
      "Accuracy: 0.54\n",
      "Accuracy: 0.54\n",
      "Accuracy: 0.53\n",
      "Accuracy: 0.54\n",
      "Accuracy: 0.54\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.57\n",
      "Accuracy: 0.57\n",
      "Accuracy: 0.57\n",
      "Accuracy: 0.57\n",
      "Accuracy: 0.57\n",
      "Accuracy: 0.57\n",
      "Accuracy: 0.57\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.54\n",
      "Accuracy: 0.54\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.54\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/scratch/lgrinszt/rlhf_experiments/minimal_example.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/scratch/lgrinszt/rlhf_experiments/minimal_example.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     batch \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/scratch/lgrinszt/rlhf_experiments/minimal_example.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/scratch/lgrinszt/rlhf_experiments/minimal_example.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     scores \u001b[39m=\u001b[39m rm(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbatch)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/scratch/lgrinszt/rlhf_experiments/minimal_example.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m delta_scores \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mdiff()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/scratch/lgrinszt/rlhf_experiments/minimal_example.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m all_delta_scores\u001b[39m.\u001b[39mextend(delta_scores\u001b[39m.\u001b[39mtolist())\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:903\u001b[0m, in \u001b[0;36mGPTNeoXForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    897\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    898\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    901\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 903\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgpt_neox(\n\u001b[1;32m    904\u001b[0m     input_ids,\n\u001b[1;32m    905\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    906\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    907\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    908\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    909\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    910\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    911\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    912\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    913\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    914\u001b[0m )\n\u001b[1;32m    915\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    916\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscore(hidden_states)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:657\u001b[0m, in \u001b[0;36mGPTNeoXModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    649\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    650\u001b[0m         create_custom_forward(layer),\n\u001b[1;32m    651\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m         head_mask[i],\n\u001b[1;32m    655\u001b[0m     )\n\u001b[1;32m    656\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 657\u001b[0m     outputs \u001b[39m=\u001b[39m layer(\n\u001b[1;32m    658\u001b[0m         hidden_states,\n\u001b[1;32m    659\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    660\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    661\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    662\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    663\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    664\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    665\u001b[0m     )\n\u001b[1;32m    666\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    667\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:420\u001b[0m, in \u001b[0;36mGPTNeoXLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, use_cache, layer_past, output_attentions)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    411\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    412\u001b[0m     hidden_states: Optional[torch\u001b[39m.\u001b[39mFloatTensor],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    418\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    419\u001b[0m ):\n\u001b[0;32m--> 420\u001b[0m     attention_layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    421\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_layernorm(hidden_states),\n\u001b[1;32m    422\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    423\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    424\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    425\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    426\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    427\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    428\u001b[0m     )\n\u001b[1;32m    429\u001b[0m     attn_output \u001b[39m=\u001b[39m attention_layer_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: attn_output, present, (attn_weights)\u001b[39;00m\n\u001b[1;32m    430\u001b[0m     attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_dropout(attn_output)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:185\u001b[0m, in \u001b[0;36mGPTNeoXAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, layer_past, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m has_layer_past:\n\u001b[1;32m    184\u001b[0m     seq_len \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m layer_past[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[0;32m--> 185\u001b[0m cos, sin \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrotary_emb(value, seq_len\u001b[39m=\u001b[39;49mseq_len)\n\u001b[1;32m    186\u001b[0m query, key \u001b[39m=\u001b[39m apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n\u001b[1;32m    187\u001b[0m query \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((query, query_pass), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:320\u001b[0m, in \u001b[0;36mGPTNeoXRotaryEmbedding.forward\u001b[0;34m(self, x, seq_len)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mif\u001b[39;00m seq_len \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_seq_len_cached:\n\u001b[1;32m    319\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_cos_sin_cache(seq_len\u001b[39m=\u001b[39mseq_len, device\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 320\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcos_cached[:seq_len, \u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m]\u001b[39m.\u001b[39;49mto(x\u001b[39m.\u001b[39;49mdevice), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msin_cached[:seq_len, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_scores, all_delta_scores, all_tokens = [], [], []\n",
    "# move dataloader to GPU\n",
    "import numpy as np\n",
    "rm.eval()\n",
    "rm = rm.to(\"cuda\")\n",
    "for batch in eval_dataloader:\n",
    "        # move batch to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            scores = rm(**batch)[0]\n",
    "\n",
    "        delta_scores = scores.reshape(-1, 2).diff().view(-1)\n",
    "        all_delta_scores.extend(delta_scores.tolist())\n",
    "        all_scores.extend(scores.view(-1).tolist())\n",
    "        all_tokens.extend(batch[\"input_ids\"].tolist())\n",
    "\n",
    "        delta_scores = np.hstack(all_delta_scores)\n",
    "        accuracy = (delta_scores > 0).mean()\n",
    "        print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
